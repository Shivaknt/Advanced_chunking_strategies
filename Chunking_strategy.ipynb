{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimise chunking strategy"
      ],
      "metadata": {
        "id": "lPCnWBQtKJbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain_experimental langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31npxGq7E5cq",
        "outputId": "a68a0d80-e535-4bfb-f66b-34cb2f63a83d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "m8-mMo6vE1o3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " #1. Character Text Splitting"
      ],
      "metadata": {
        "id": "aIkfSVCKJFxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Character Text Splitting\n",
        "print(\"#### Character Text Splitting ####\")\n",
        "\n",
        "text = \"Text splitting in LangChain is a critical feature that facilitates the division of large texts into smaller, manageable segments. \"\n",
        "\n",
        "# Manual Splitting\n",
        "chunks = []\n",
        "chunk_size = 35 # Characters\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5To4axieEqIP",
        "outputId": "2435fa97-cb51-40e5-8318-5bb0042e9157"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Character Text Splitting ####\n",
            "[Document(page_content='Text splitting in LangChain is a cr', metadata={'source': 'local'}), Document(page_content='itical feature that facilitates the', metadata={'source': 'local'}), Document(page_content=' division of large texts into small', metadata={'source': 'local'}), Document(page_content='er, manageable segments. ', metadata={'source': 'local'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Text Splitting"
      ],
      "metadata": {
        "id": "c0Tjvx7tJKyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatic Text Splitting\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
        "documents = text_splitter.create_documents([text])\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVggPbr_EqGD",
        "outputId": "b18970a4-9378-429d-ec9a-6e0cfec70bb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Text splitting in LangChain is a cr'), Document(page_content='itical feature that facilitates the'), Document(page_content=' division of large texts into small'), Document(page_content='er, manageable segments. ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Recursive Character Text Splitting"
      ],
      "metadata": {
        "id": "uUKXBzPuJM1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Recursive Character Text Splitting\n",
        "print(\"#### Recursive Character Text Splitting ####\")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
        "print(text_splitter.create_documents([text]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYbx_PUbEqAG",
        "outputId": "c00f75f5-d806-4668-b8b5-5baea4a5f6ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Recursive Character Text Splitting ####\n",
            "[Document(page_content='Text splitting in LangChain is a critical feature that'), Document(page_content='facilitates the division of large texts into smaller, manageable'), Document(page_content='segments.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Document Specific Splitting"
      ],
      "metadata": {
        "id": "0zwSLazSJPHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Document Specific Splitting\n",
        "print(\"#### Document Specific Splitting ####\")\n",
        "\n",
        "# Document Specific Splitting - Markdown\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
        "markdown_text = \"\"\"\n",
        "# Fun in California\n",
        "\n",
        "## Driving\n",
        "\n",
        "Try driving on the 1 down to San Diego\n",
        "\n",
        "### Food\n",
        "\n",
        "Make sure to eat a burrito while you're there\n",
        "\n",
        "## Hiking\n",
        "\n",
        "Go to Yosemite\n",
        "\"\"\"\n",
        "print(splitter.create_documents([markdown_text]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYsuyoxSEp8K",
        "outputId": "50316686-dc49-4d65-d9f7-685ac6e539f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Document Specific Splitting ####\n",
            "[Document(page_content='# Fun in California\\n\\n## Driving'), Document(page_content='Try driving on the 1 down to San Diego'), Document(page_content='### Food'), Document(page_content=\"Make sure to eat a burrito while you're\"), Document(page_content='there'), Document(page_content='## Hiking\\n\\nGo to Yosemite')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "0alg_DEZGMEk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Semantic Chunking"
      ],
      "metadata": {
        "id": "knokwtWMJ6b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Semantic Chunking\n",
        "print(\"#### Semantic Chunking ####\")\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Percentile - all differences between sentences are calculated, and then any difference greater than the X percentile is split\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
        "text_splitter = SemanticChunker(\n",
        "    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
        ")\n",
        "documents = text_splitter.create_documents([text])\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh3N-EOGEp3y",
        "outputId": "d886f5a5-bd98-4a59-e099-c4515c5a0939"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Semantic Chunking ####\n",
            "[Document(page_content='Text splitting in LangChain is a critical feature that facilitates the division of large texts into smaller, manageable segments. ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Agentic Chunking"
      ],
      "metadata": {
        "id": "GUQsyURmKTGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Agentic Chunking\n",
        "print(\"#### Proposition-Based Chunking ####\")\n",
        "\n",
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from typing import Optional, List\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain import hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMyhPSN1Ep1e",
        "outputId": "341479db-932e-464c-93f6-a109a4718159"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Proposition-Based Chunking ####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJq5sKINIcB4",
        "outputId": "f6cf984a-f006-47f8-a634-f1b74cdc5c05"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "runnable = obj | llm"
      ],
      "metadata": {
        "id": "aE8ns3lAEpzG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentences(BaseModel):\n",
        "    sentences: List[str]\n",
        "\n",
        "# Extraction\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n",
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({\n",
        "    \t\"input\": text\n",
        "    }).content\n",
        "    propositions = extraction_chain.invoke(runnable_output)[\"text\"][0].sentences\n",
        "    return propositions\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "text_propositions = []\n",
        "for i, para in enumerate(paragraphs[:5]):\n",
        "    propositions = get_propositions(para)\n",
        "    text_propositions.extend(propositions)\n",
        "    print (f\"Done with {i}\")\n",
        "\n",
        "print (f\"You have {len(text_propositions)} propositions\")\n",
        "print(text_propositions[:10])\n",
        "\n",
        "print(\"#### Agentic Chunking ####\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhAORlAIEpt6",
        "outputId": "36734971-273e-4a5c-f269-573a037740ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with 0\n",
            "You have 4 propositions\n",
            "['Text splitting in LangChain is a critical feature.', 'Text splitting facilitates the division of large texts into smaller, manageable segments.', 'Text splitting is a critical feature in LangChain.', 'Text splitting facilitates the division of large texts into smaller, manageable segments in LangChain.']\n",
            "#### Agentic Chunking ####\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}